import ollama
import os
import re

HOST = os.environ.get("LLAMA_HOST", "http://localhost:11434")
MODEL = os.environ.get("LLAMA_MODEL", "qwen2.5-coder:7b")
MODELNAME='raven'
MODELFILE='''
FROM qwen2.5-coder:7b
SYSTEM you should only ouput the necessary dockercompose.yml and nothing else. no verbose output. Do not use build if no dockerfile was given in the input. install necessary dependencies'''

client = ollama.Client(
  host=HOST
)
ollama.create(model=MODELNAME, modelfile=MODELFILE)

NO_PRIV_PORT =  ''' 
IF YOU ARE USING A WEB SERVER suck as nginx, traeiffik Do
not use any privilaged ports in the host machine example instead of using 80:80
portmapping use 8080:80 and output the url nginx, or web hosting. 
'''

def gen_ai_output(content, flag = False):

    response = client.generate(
        model=MODELNAME,
        prompt=content,
    )

    output = response['response']
    if 'nginx' in output and flag == False:
        print('[WARN]: nginix detected running on non priv port')
        return gen_ai_output(content + NO_PRIV_PORT, flag=True)
    return output

def gen_yaml(content):

    print("[LOG]: AI | CREATING DOCKER COMPOSE YAML...")
    ai_output = gen_ai_output(content)
    try:
        foo = re.compile(r"(```yaml\n)([\S\s]*)(```)")
        bar = re.search(foo, ai_output)
        tmp = bar.groups()[1]

        docker_yaml = ''
        docker_yaml += "# Generated by raven cli tool build by raxen and vasanth"
        docker_yaml += "\n"
        docker_yaml += tmp
        docker_yaml = re.sub(r"version:.*\n", '', docker_yaml)
        print("[LOG]: ", docker_yaml)
        return docker_yaml
    except:
        print("[ERROR]: AI | DOCKER COMPOSE CREATION FAILED | Trying again...")
        return gen_yaml(content)




